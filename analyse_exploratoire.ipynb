{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ks9jq3yPQwUJ"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import chi2_contingency\n","import scipy.stats as stats\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","\n","          # analyse unidimentionelle quantitative\n","\"\"\"\n","  params: a dataframe \n","  return : generating a set of descriptive statistics for a DataFrame or a Series\n","\"\"\"\n","def statistics_for_numeric_variables(data): \n","  return data.describe()\n","\n","\"\"\"\n","  The function displays the boxplot of a quantitative variable\n","  params : numeric variable of a dataframe\n","\"\"\"\n","def display_boxplot(col):\n","    sns.boxplot(data=col).set(xlabel=col.name)\n","    mean = col.mean()\n","    plt.axhline(y=mean, color='r', linestyle='--', label='Mean')\n","    plt.legend()\n","    plt.show()\n","\n","\"\"\"\n","  la fonction affiche l'histogramme d'une variable numerique\n","  params : numeric variable of a dataframe\n","\"\"\"\n","def histogramme(numeric_column):\n","  plt.hist(numeric_column, bins=10)\n","  plt.title('Distribution de la variable',numeric_column.name)\n","  plt.xlabel(numeric_column.name)\n","  plt.ylabel('Fréquence')\n","  plt.show()\n","\n","\n","\n","          # analyse unidimensionnelle qualitative:\n","\n","#Identifier les catégories de la variable qualitative\n","def identify_categories(column):\n","  return column.unique()\n","\n","#Calculer et representation graphique des pourcentages de chaque variable catégorielle\n","\"\"\"\n","  the function returns a pie plot representing the percentage of each category of a qualitative variable\n","  params : column of categorical variable \n","  return : pie plot\n","\"\"\"\n","def pie_plot_of_categories(column):\n","  result=column.value_counts().apply(lambda x: x*column.shape[0]/100).to_dict()\n","  return plt.pie(result.values(), labels=result.keys(), autopct='%1.1f%%',shadow=True)\n","\n","def frequency_table():\n","  return df[\"color\"].value_counts().to_frame()\n","\n","\n","\"\"\"\n","    the function calculates the frequencies of a categorical variable, sorts the modalities in decreasing order of frequency,\n","   then calculation of the cumulative frequencies and the percentages of cumulative frequencies to after display a graph \n","   representing the cumulative frequencies\n","   params : column of categorical variable\n","\"\"\"\n","def pareto_diagram(categorical_column):\n","  freq = categorical_column.value_counts()\n","  freq = freq.sort_values(ascending=False)\n","  cumfreq = freq.cumsum()\n","  totalfreq = sum(freq)\n","  percentcumfreq = (cumfreq/totalfreq)*100\n","\n","  fig, ax1 = plt.subplots()\n","  ax1.bar(freq.index, freq.values, color='b')\n","  ax1.set_xlabel('Modalité')\n","  ax1.set_ylabel('Fréquence', color='b')\n","  ax1.tick_params(axis='y', labelcolor='b')\n","  # Création de la ligne de fréquence cumulée\n","  ax2 = ax1.twinx()\n","  ax2.plot(freq.index, percentcumfreq, color='r', marker='o')\n","  ax2.set_ylabel('Pourcentage de fréquence cumulée', color='r')\n","  ax2.tick_params(axis='y', labelcolor='r')\n","\n","  plt.title('Diagramme de Pareto')\n","  plt.show()\n","\n","\n","              # analyse bidimensionnelle\n","              # qualitative * qualitative\n","\n","\n","\"\"\"\n","  The function builds the contingency table from the variables passed as parameters then displays the information returned by the latter in a bar plot\n","  params : represents columns of the dataframe\n","\"\"\"\n","def graphic_representation(categorical_variable_1,categorical_variable_2):\n","  contingency_table = pd.crosstab(categorical_variable_1, categorical_variable_2)\n","\n","  # Création du diagramme en barres empilées\n","  contingency_table.plot(kind='bar', stacked=True)\n","  plt.title('Tableau de contingence entre',categorical_variable_1,' et ',categorical_variable_2)\n","  plt.xlabel('Variable1')\n","  plt.ylabel('Nombre d\\'observations')\n","  plt.show()\n","\n","# analyse bidimensionnelle sur les variables qualitatives\n","\n","# test du chi-carré  et Le coefficient de contingence sont utiliséq pour 2 variables qualitatives\n","\n","#Le test du chi-carré : est une méthode statistique utilisée pour évaluer l'indépendance entre deux variables qualitatives. \n","#Le test du chi-carré compare les fréquences observées dans un tableau de contingence avec les fréquences attendues si les deux \n","#variables étaient indépendantes. Si les fréquences observées sont significativement différentes des fréquences attendues, cela indique \n","#une relation entre les variables.\n","\n","#Le coefficient de contingence  cramer : est une mesure de la force de la relation entre deux variables qualitatives. Le coefficient de contingence\n","# varie de 0 à 1, où 0 indique l'absence de relation et 1 indique une relation forte.\n","\n","#Le Khi2 ici nous indique donc qu’il existe une liaison entre les deux variables ; \n","# le V de Cramer nous indique que cette liaison est très forte par sa valeur élevée.\n","# tableau de contingence entre 2 variable categorielles\n","\"\"\"\n","  The function from the contingency table of the 2 variables passed as a parameter, calculates the test of χ2 which indicates the link between the two quantitative variables\n","  params : represents columns of the dataframe\n","  return : the contingency table as well as the value of the χ2 and the p-value\n","\"\"\"\n","def contingency_table_chi2_contengency_coefficent(categorical_variable_1,categorical_variable_2):\n","\n","  contingency_table=pd.crosstab(categorical_variable_1, categorical_variable_2)\n","  chi2, p_value, degres_liberte, _ = chi2_contingency(contingency_table)\n","  #contengency_coefficent=pg.cramers_v(contingency_table.values)\n","\n","  return contingency_table, chi2,p_value\n","\n","# χ2 = 0 si X et Y sont totalement indépendantes \n","# χ2 est d’autant plus grand que la liaison entre X et Y est forte\n"," # ou bien on verifie par rapport à la p-value si p-value <= 5% x a un effet significatif sur Y\n"," # si p-value > 5% x n'a pas d'effet sur Y\n","\n","\n","               # qualitative * quantitative\n","\n","#calculer la moyenne et l'écart-type de la variable quantitative pour chaque catégorie de la variable catégorielle\n","\n","def mean_and_std_by_categorical_variable(categorical_variable,data):# data est un dataframe avec une 2 colonnes une categorielle et lautre numerique\n","# categorical_variable c'est le nom de la variable seulement \n","  means = df.groupby(categorical_variable).mean()\n","  means=means.rename(columns={means.columns[0]:\"mean\"})\n","\n","  stds = df.groupby(categorical_variable).std()\n","  stds=stds.rename(columns={stds.columns[0]:\"std\"})\n","\n","  return pd.concat([means,stds],axis=1)\n","  \n","# data must have the categorical variable and the numeric variable that we want to calculate the fisher indicator\n"," # lien à utiliser pour (table de distribution): http://www.socr.ucla.edu/Applets.dir/F_Table.html avec df1= df_within et df2=df_within pour alpha=0,05\n"," #https://towardsdatascience.com/statistics-in-python-using-anova-for-feature-selection-b4dc876ef4f0\n"," # si l'indicateur de fisher < s5%(k ,n ), on conclura que x(categoriel) n'a pas d'effet significatif sur Y (on accepte l'hypothese nulle) => le x n'est pas inclu dans les features\n"," # si l'indicateur de fisher > s5%(k ,n ), on conclura que x(categoriel) a un effet significatif sur Y (on rejete l'hypothese nulle) => le x sera inclu dans les features\n"," # ou bien on verifie par rapport à la p-value si p-value <= 5% x a un effet significatif sur Y\n"," # si p-value > 5% x n'a pas d'effet sur Y\n"," \n","def fisher_indicator(categorical_variable,data,index): # data contient la variable categorielle et quantitative voulu et categorical_variable c'est le nom de la variable catégoriels \n","  nb_categories=len(identify_categories(data[categorical_variable]))\n","  df=data.pivot(columns=categorical_variable, index=index) \n","  df_within = df.shape[0] -  df.shape[1]      \n","  df_between = df.shape[1] - 1 \n","  fvalue, pvalue = stats.f_oneway( \n","      *df.iloc[:,0:nb_categories].T.values)\n","  return pvalue \n","\n","\n","\"\"\"\n","  La fonction affiche un ou plusieurs boxplots representant la distribution de la variable quantitative en fonction des differentes categories d'une variable qualitative \n","  params :numeric_variable, categorical_variable : colonnes du dataframe\n","\n","\"\"\"\n","\n","def box_plot_of_variable_according_to_categorical_variable(numeric_variable,categorical_variable):\n"," return sns.boxplot(x=categorical_variable, y=numeric_variable, data=data,showmeans=True, meanprops={\"color\":\"green\"})\n","\n","\n","               # quantitative * quantitative\n","\n","\"\"\"\n","  The function displays a scatter plot of 2 quantitative variables that graphically shows whether there is a correlation between them or not\n","  params : quantitative variables representing columns of the dataset\n","\"\"\"\n","def scatter_pot(x,y):\n","  fig, ax = plt.subplots(figsize=(10, 5))\n","  plt.plot(x,y,\"ob\") # ob = type de points \"o\" ronds, \"b\" bleus\n","  plt.title(\"Titre du graphique\")\n","  plt.show()\n","\n","\"\"\"\n","  the function displays the heatmap between 2 or more quatitative variables which represents the level of collinearity between the variables\n","  params : represents the dataframe with the quantitative variables\n","\"\"\"\n","def heatmap_plot(data):\n","  colormap = sns.color_palette(\"Greens\")\n","  graph = sns.heatmap(data.corr(), annot=True, cmap=colormap)\n","  plt.show(graph.figure)\n","\n","  # permet de faire la matrice de correlation\n","#La matrice de corrélation est un outil d'analyse de données qui permet d'analyser les relations linéaires entre deux ou plusieurs variables numériques\n","#Les coefficients de corrélation varient entre -1 et 1, où -1 indique une corrélation négative parfaite, 0 indique une absence de corrélation et 1 indique \n","#une corrélation positive parfaite. Les coefficients de corrélation peuvent être calculés à l'aide de la corrélation de Pearson,\n","def correlation_matrix(data):\n","  return data.corr()\n","\n","\n","def relation_between_variables(p_value,threshold):\n","  if abs(p_value) <= threshold : # les variables sont dependantes\n","    return True\n","  else:\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import RepeatedStratifiedKFold, KFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import tree\n","import scikitplot as skplt\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","from sklearn.metrics import roc_curve\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n","\n","def hyper_params_selection(method,algorithm,params):\n","  #cv = KFold(n_splits=10, shuffle=True, random_state=42)\n","  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n","  if method==\"GridSearchCV\": \n","    grid_search = GridSearchCV(estimator=algorithm, param_grid=params, n_jobs=-1, cv=cv, scoring=\"accuracy\",error_score=0)\n","  elif method==\"RandomizedSearchCV\":\n","    grid_search=RandomizedSearchCV(algorithm, params, scoring='accuracy', cv=10)   \n","  return grid_search\n","                        \n","\n","def load_dataset(data,target,test_size):\n","  input=data.loc[:, data.columns != target.name]\n","  X_train,X_test,y_train,y_test = train_test_split(input,target,test_size=test_size,random_state=42, stratify=target.values)\n","  return X_train,X_test,y_train,y_test\n","\n","def knn_model():\n","  return KNeighborsClassifier()\n","\n","def model_with_params(grid_result,algorithm):\n","  if algorithm == \"knn\":\n","    return KNeighborsClassifier(n_neighbors=grid_result.best_params_['n_neighbors'],weights=grid_result.best_params_[\"weights\"],algorithm=grid_result.best_params_[\"algorithm\"],metric=grid_result.best_params_[\"metric\"],p=grid_result.best_params_[\"p\"])\n","  elif algorithm == \"logisticRegression\":\n","    return \"\"\n","  elif algorithm == \"DecisionTree\":\n","    return \"\"\n","\n","def _params(algorithm):\n","  if algorithm == \"knn\":\n","    n_neighbors = range(1, 21, 2)\n","    weights = ['uniform', 'distance']\n","    metric = ['euclidean', 'manhattan', 'minkowski']\n","    algorithm = ['auto','ball_tree','kd_tree','brute']\n","    p= [1, 2]\n","    params = dict(n_neighbors=n_neighbors,weights=weights,metric=metric,algorithm=algorithm,p=p)\n","  if algorithm==\"logisticRegression\":\n","    params={'C': [0.1, 1, 10], \n","     'penalty': ['l1', 'l2','elasticnet'],\n","     'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n","     }\n","\n","  return params\n","\n","def model_without_params(algorithm):\n","    if algorithm == \"knn\":\n","      return KNeighborsClassifier()\n","    elif algorithm == \"logisticRegression\":\n","      return LogisticRegression()\n","    elif algorithm == \"DecisionTree\":\n","      return tree.DecisionTreeClassifier()\n","\n","def heuristic_method(train,method_name,X_train,y_train):\n","  if method_name==\"OvO\":\n","    ovo = OneVsOneClassifier(train)\n","    ovo.fit(X_train, y_train)\n","    return ovo\n","  elif method_name==\"OvR\":\n","    ovr = OneVsRestClassifier(train)\n","    ovr.fit(X_train, y_train)\n","    return ovr\n","  elif method_name==\"\":\n","    fitted_data=train.fit(X_train, y_train)\n","    return fitted_data\n","\n","# data represente tout le dataset\n","# model represente l'appel de l'algorithme dans les parametre\n","# test_size represente la taille du du test set est c'est 0.89.. qlq chose\n","# heuristic_method represente la methode à utiliser pour diviser les données de la target \n","# target represente la classe output le y \n","# selected_algorithm represente le nom de l'algorithme à utiliser \n","# hyper_params_method represente la methode utiliser pour faire la bonne selection des hyperparametre\n","\n","def training(target,selected_algorithm,test_size,hyper_params_method,encoding_method,data):# encoding_method : si on est pas en multiclasse \n","                                                                                          # on va mettre une chaine vide\n","\n","  X_train,X_test,y_train,y_test=load_dataset(data,target,test_size)\n","  model=model_without_params(selected_algorithm)\n","  grid=_params(selected_algorithm)\n","  grid_search=hyper_params_selection(hyper_params_method,model,grid)\n","  grid_result = grid_search.fit(X_train, y_train)\n","\n","  train=model_with_params(grid_result,selected_algorithm)\n","  fitted_data= heuristic_method(train,encoding_method,X_train,y_train)\n","\n","  return fitted_data,X_test,y_test\n","\n","\n","def prediction(fitted_data,X_test):# ovr ou ovo + X_test\n","  y_pred = fitted_data.predict(X_test)\n","  return y_pred\n","\n","# Ce score est une mesure de la performance du modèle, qui représente le coefficient de détermination R². Le coefficient de\n","# détermination R² est une mesure de l'adéquation du modèle aux données\n","def score(X_test, y_test,fitted_data):\n","  return fitted_data.score(X_test, y_test)\n","\n","def comparing_results(X_test,y_test,y_pred):\n","  df = X_test.copy()\n","  row_number=df.shape[1]+1\n","  df.insert(df.shape[1],\"Actual\",y_test, True)\n","  df.insert(row_number,\"Predicted\",y_pred, True)\n","  return df\n","\n","def confusion_matrix(y_test,y_pred):\n","  skplt.metrics.plot_confusion_matrix(y_test,y_pred)\n","\n","def rocCurve(X_test,y_test):\n","  #recall = TPR et specificity = FPR\n","  fpr, tpr, thresholds = roc_curve(y_test, fitted_data.predict_proba(X_test)[:,1])\n","  print(\"recall\",tpr)\n","  print(\"specificity\",1 - fpr)\n","  roc_df = pd.DataFrame({'recall': tpr, 'specificity': 1 - fpr})\n","\n","  ax = roc_df.plot(x='specificity', y='recall', figsize=(4, 4), legend=False)\n","  ax.set_ylim(0, 1)\n","  ax.set_xlim(1, 0)\n","  # ax.plot((1, 0), (0, 1))\n","  ax.set_xlabel('specificity')\n","  ax.set_ylabel('recall')\n","  ax.fill_between(roc_df.specificity, 0, roc_df.recall, alpha=0.3)\n","  \n","  plt.tight_layout()\n","  plt.show()\n","\n","\n","# evaluation \n","\n","# y a que l'accuracy qui est definie comme metrique pour le binaire et le mutliclasses\n","def evaluation(y_test,y_pred):\n","  accuracy=accuracy_score(y_test, y_pred)\n","  report_f1_recall_precision=classification_report(y_test, y_pred)\n","  if len(set(y_pred))<=2:\n","    curve_roc=rocCurve(X_test,y_test)\n","    recall=recall_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    return accuracy,report_f1_recall_precision,curve_roc,recall,precision\n","  else:\n","    #La macro-precision est une métrique utilisée pour évaluer les performances d'un modèle de classification multi-classes. \n","    #Elle calcule la précision pour chaque classe individuellement et calcule ensuite la moyenne de ces précisions pour donner une \n","    #mesure globale de la performance du modèle.\n","    # si on a un desiquilibre des classes vaut mieux utiliser micro-precision sinon macro\n","    # Une valeur de macro-precision élevée indique que le modèle est capable de classer correctement un grand nombre de classes différentes\n","    precision=precision_score(y_test, y_pred, average=\"macro\") \n","    \n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPfu8K38WO5D8UebR3Fu7zL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
